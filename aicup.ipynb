{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install datasets"
      ],
      "metadata": {
        "id": "iDaHQrOkhzPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "AYY9gn0T6BZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qMO95olzhlts"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "plm = \"EleutherAI/pythia-410m-deduped\"\n",
        "\n",
        "bos = '<|endoftext|>'\n",
        "eos = '<|END|>'\n",
        "pad = '<|pad|>'\n",
        "sep ='\\n\\n####\\n\\n'\n",
        "\n",
        "special_tokens_dict = {'eos_token': eos, 'bos_token': bos, 'pad_token': pad, 'sep_token': sep}\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(plm, revision=\"step3000\")\n",
        "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
        "tokenizer.padding_side = 'left'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, Features, Value\n",
        "dataset = load_dataset(\"csv\", data_files=\"/content/drive/MyDrive/Colab Notebooks/opendid_valid/opendid_set1.tsv\", delimiter='\\t',\n",
        "                       features = Features({\n",
        "                              'fid': Value('string'), 'idx': Value('int64'),\n",
        "                              'content': Value('string'), 'label': Value('string')}),\n",
        "                       column_names=['fid', 'idx', 'content', 'label'], keep_default_na=False)\n",
        "dataset"
      ],
      "metadata": {
        "id": "DJ99aKVciAFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "sub_datasets = torch.utils.data.random_split(dataset['train'], [164495,0])\n",
        "print(len(sub_datasets[0]))\n",
        "for i in range(4): print(sub_datasets[0][i])"
      ],
      "metadata": {
        "id": "4N0RLrxgika7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PAD_IDX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
        "IGNORED_PAD_IDX = -100\n",
        "PAD_IDX"
      ],
      "metadata": {
        "id": "dDGed3ZMkAtd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "\n",
        "train_data = list(sub_datasets[0])\n",
        "\n",
        "def collate_batch(batch):\n",
        "    texts = [f\"{bos} {data['content']} {sep}\"+ data['label'].replace('\\\\n','\\n')+f\" {eos}\" for data in list(batch)] # 範例 prompt\n",
        "    encoded_seq = tokenizer(texts, padding=True)\n",
        "\n",
        "    indexed_tks = torch.tensor(encoded_seq['input_ids'])\n",
        "    attention_mask = torch.tensor(encoded_seq['attention_mask'])\n",
        "    encoded_label = torch.tensor(encoded_seq['input_ids'])\n",
        "    encoded_label[encoded_label == tokenizer.pad_token_id] = IGNORED_PAD_IDX\n",
        "\n",
        "    return indexed_tks, encoded_label, attention_mask\n",
        "\n",
        "train_dataloader = DataLoader(train_data, batch_size=2, shuffle=False, collate_fn=collate_batch)\n",
        "titer = iter(train_dataloader)\n",
        "tks, labels, masks= next(titer)\n",
        "print(tks.shape)\n",
        "next(iter(titer))"
      ],
      "metadata": {
        "id": "8lXKk6_9jep9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "BATCH_SIZE = 6\n",
        "\n",
        "class BatchSampler():\n",
        "    def __init__(self, data, batch_size):\n",
        "        self.pooled_indices = []\n",
        "        self.data = data\n",
        "        self.batch_size = batch_size\n",
        "        self.len = len(list(data))\n",
        "    def __iter__(self):\n",
        "        self.pooled_indices = []\n",
        "        indices = [(index, len(data[\"content\"])) for index, data in enumerate(self.data)]\n",
        "        random.shuffle(indices)\n",
        "        for i in range(0, len(indices), BATCH_SIZE * 100):\n",
        "            self.pooled_indices.extend(sorted(indices[i:i + BATCH_SIZE * 100], key=lambda x: x[1], reverse=True))\n",
        "        self.pooled_indices = [x[0] for x in self.pooled_indices]\n",
        "\n",
        "        for i in range(0, len(self.pooled_indices), BATCH_SIZE):\n",
        "            yield self.pooled_indices[i:i + BATCH_SIZE]\n",
        "    def __len__(self):\n",
        "        return (self.len + self.batch_size - 1) // self.batch_size\n",
        "\n",
        "bucket_train_dataloader = DataLoader(train_data, batch_sampler=BatchSampler(train_data, BATCH_SIZE),\n",
        "                                     collate_fn=collate_batch, pin_memory=True)"
      ],
      "metadata": {
        "id": "mD4E8A8vj7ym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoConfig\n",
        "\n",
        "config = AutoConfig.from_pretrained(plm,\n",
        "                                    bos_token_id=tokenizer.bos_token_id,\n",
        "                                    eos_token_id=tokenizer.eos_token_id,\n",
        "                                    pad_token_id=tokenizer.pad_token_id,\n",
        "                                    sep_token_id=tokenizer.sep_token_id,\n",
        "                                    output_hidden_states=False)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(plm, revision=\"step3000\", config=config)\n",
        "model"
      ],
      "metadata": {
        "id": "qARsuo7MkuVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "from torch.nn import functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "Wi_FTZKclXb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "from torch.optim import AdamW\n",
        "import os\n",
        "\n",
        "EPOCHS = 6\n",
        "optimizer = AdamW(model.parameters(),lr=5e-5)\n",
        "\n",
        "steps = len(bucket_train_dataloader)\n",
        "total_steps = steps * EPOCHS\n",
        "print(steps, total_steps)\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=total_steps*0.1,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "model.to(device)\n",
        "print(f'Total numbers of steps: {total_steps}')\n",
        "model\n"
      ],
      "metadata": {
        "id": "YGDLk0M2lchF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm,trange\n",
        "# 模型儲存路徑\n",
        "model_dir = \"/content/drive/MyDrive/Colab Notebooks/opendid_valid/model\"\n",
        "if not os.path.isdir(model_dir):\n",
        "    os.mkdir(model_dir)\n",
        "\n",
        "min_loss = 9999\n",
        "global_step = 0\n",
        "total_loss = 0\n",
        "\n",
        "model.train()\n",
        "for _ in trange(EPOCHS, desc=\"Epoch\"):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    predictions , true_labels = [], []\n",
        "\n",
        "    for step, (seqs, labels, masks) in enumerate(bucket_train_dataloader):\n",
        "        seqs = seqs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        masks = masks.to(device)\n",
        "        model.zero_grad()\n",
        "        outputs = model(seqs, labels=labels ,attention_mask=masks)\n",
        "        logits = outputs.logits\n",
        "        loss = outputs.loss\n",
        "        loss = loss.mean()\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()  # 更新學習率\n",
        "    avg_train_loss = total_loss / len(bucket_train_dataloader)\n",
        "    print(\"Average train loss: {}\".format(avg_train_loss))\n",
        "    torch.save(model.state_dict(), os.path.join(model_dir , 'GPT_Finial.pt'))\n",
        "    if avg_train_loss < min_loss:\n",
        "        min_loss = avg_train_loss\n",
        "        torch.save(model.state_dict(), os.path.join(model_dir , 'GPT_best.pt'))"
      ],
      "metadata": {
        "id": "FHzppiyllreQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_text(model, tokenizer, seed, n_words=20):\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    text = tokenizer.encode(seed)\n",
        "    inputs, past_key_values = torch.tensor([text]), None\n",
        "    with torch.no_grad():\n",
        "        for _ in tqdm(range(n_words)):\n",
        "            out = model(inputs.to(device), past_key_values=past_key_values)\n",
        "            logits = out.logits\n",
        "            past_key_values = out.past_key_values\n",
        "            log_probs = F.softmax(logits[:, -1], dim=-1)\n",
        "            inputs = torch.multinomial(log_probs, 1)\n",
        "            text.append(inputs.item())\n",
        "            if tokenizer.decode(inputs.item()) == eos:\n",
        "                break\n",
        "\n",
        "\n",
        "    return tokenizer.decode(text)\n",
        "\n",
        "sample_text(model, tokenizer, seed=f\"{bos} D.O.B:  28/6/2003 {sep}\")"
      ],
      "metadata": {
        "id": "YeG29vRlcqqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, Features, Value\n",
        "valid_data = load_dataset(\"csv\", data_files=\"/content/drive/MyDrive/opendid_valid/opendid_test.tsv\", delimiter='\\t',\n",
        "                          features = Features({\n",
        "                              'fid': Value('string'), 'idx': Value('int64'),\n",
        "                              'content': Value('string'), 'label': Value('string')}),\n",
        "                              column_names=['fid', 'idx', 'content', 'label'])\n",
        "valid_list= list(valid_data['train'])\n",
        "valid_list"
      ],
      "metadata": {
        "id": "_FVq6xqQls1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "model_path=os.path.join(\"/content/drive/MyDrive/Colab Notebooks/opendid_valid/model/GPT_best.pt\")\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "model = model.to(device)\n",
        "tokenizer.padding_side = 'left'\n",
        "def sample_batch(model, tokenizer, input):\n",
        "    \"\"\"Generate text from a trained model.\"\"\"\n",
        "    model.eval()\n",
        "    seeds = [f\"{bos} {text['content']} {sep}\" for text in input]\n",
        "    texts = tokenizer(seeds, return_tensors = 'pt', padding=True).to(device)\n",
        "    outputs = []\n",
        "    #return\n",
        "    with torch.cuda.amp.autocast():\n",
        "        output_tokens = model.generate(**texts, max_new_tokens=420, pad_token_id = PAD_IDX,\n",
        "                                        eos_token_id=tokenizer.convert_tokens_to_ids(eos))\n",
        "        preds = tokenizer.batch_decode(output_tokens)\n",
        "        for idx , pred in enumerate(preds):\n",
        "            pred = pred[pred.index(sep)+len(sep):].replace(pad, \"\").replace(eos, \"\").strip()\n",
        "            if pred == \"PHI: NULL\":\n",
        "                continue\n",
        "            phis = pred.split('\\n')\n",
        "            lidxs = {}\n",
        "            for p in phis:\n",
        "                tid = p.find(':')\n",
        "                if tid > 0:\n",
        "                    text = p[tid+1:].strip()\n",
        "                    nv = text.find('=>')\n",
        "                    normalizedV = None\n",
        "                    # 處理時間正規化\n",
        "                    if nv>0:\n",
        "                        normalizedV = text[nv+2:]\n",
        "                        text = text[:nv]\n",
        "                    # ...............\n",
        "                    lidx = 0\n",
        "                    if text in lidxs:\n",
        "                        lidx = lidxs[text]\n",
        "                    lidx = input[idx]['content'].find(text, lidx)\n",
        "                    eidx = lidx+len(text)\n",
        "                    lidxs[text] = eidx\n",
        "                    sidx=int(input[idx]['idx'])\n",
        "                    if normalizedV is None:\n",
        "                        outputs.append(f'{input[idx][\"fid\"]}\\t{p[:tid]}\\t{lidx+sidx}\\t{eidx+sidx}\\t{text}')\n",
        "                    else:\n",
        "                        outputs.append(f'{input[idx][\"fid\"]}\\t{p[:tid]}\\t{lidx+sidx}\\t{eidx+sidx}\\t{text}\\t{normalizedV}')\n",
        "    return outputs\n",
        "\n",
        "f = open(\"answer.txt\", \"w\", encoding='utf-8')\n",
        "BATCH_SIZE = 8\n",
        "for i in tqdm(range(0, len(valid_list), BATCH_SIZE)):\n",
        "    with torch.no_grad():\n",
        "        seeds = valid_list[i:i+BATCH_SIZE]\n",
        "        outputs = sample_batch(model, tokenizer, input=seeds)\n",
        "        for o in outputs:\n",
        "            f.write(o)\n",
        "            f.write('\\n')\n",
        "f.close()"
      ],
      "metadata": {
        "id": "u4nbVxCYm8Ei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = 'answer.txt'\n",
        "train_phi_category = ['PATIENT', 'DOCTOR', 'USERNAME', 'PROFESSION', 'ROOM',\n",
        "            'DEPARTMENT', 'HOSPITAL', 'ORGANIZATION', 'STREET',\n",
        "            'CITY', 'STATE', 'COUNTRY', 'ZIP', 'LOCATION-OTHER', 'AGE', 'DATE',\n",
        "            'TIME', 'DURATION', 'SET', 'PHONE', 'FAX', 'EMAIL', 'URL', 'IPADDR', 'SSN',\n",
        "            'MEDICALRECORD', 'HEALTHPLAN', 'ACCOUNT', 'LICENSE', 'VEHICLE', 'DEVICE', 'BIOID', 'IDNUM']\n",
        "\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "for line_number, line in enumerate(lines, 1):\n",
        "    label = line.split('\\t')[1].strip()\n",
        "    if label not in train_phi_category:\n",
        "        print(f\"Line {line_number}: {label}\")\n"
      ],
      "metadata": {
        "id": "xgJkIwQn1tWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = 'answer.txt'\n",
        "file_path1 = 'answer1.txt\n",
        "train_phi_category = ['PATIENT', 'DOCTOR', 'USERNAME', 'PROFESSION', 'ROOM',\n",
        "            'DEPARTMENT', 'HOSPITAL', 'ORGANIZATION', 'STREET',\n",
        "            'CITY', 'STATE', 'COUNTRY', 'ZIP', 'LOCATION-OTHER', 'AGE', 'DATE',\n",
        "            'TIME', 'DURATION', 'SET', 'PHONE', 'FAX', 'EMAIL', 'URL', 'IPADDR', 'SSN',\n",
        "            'MEDICALRECORD', 'HEALTHPLAN', 'ACCOUNT', 'LICENSE', 'VEHICLE', 'DEVICE', 'BIOID', 'IDNUM']\n",
        "\n",
        "# 讀取文件並篩選標籤在訓練標籤列表\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "filtered_lines = []\n",
        "for line in lines:\n",
        "    label = line.split('\\t')[1].strip()\n",
        "    if label in train_phi_category:\n",
        "        filtered_lines.append(line)\n",
        "\n",
        "# 將篩選後重新寫入文件中\n",
        "with open(file_path1, 'w', encoding='utf-8') as file:\n",
        "    file.writelines(filtered_lines)"
      ],
      "metadata": {
        "id": "a5y_8-T3XkSm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}